---
title: "Predicting Metacritic Categories"
description: "WORK IN PROGRESS, WE TO IDENTIFY FEATURES"
author: "Brian Cervantes Alvarez"
date: "07-03-2023"
format:
  html:
    toc: true
    toc-location: right
    html-math-method: katex
    page-layout: full
execute: 
  warning: false
  message: false
categories: [Python, Machine Learning, NLP]
jupyter: python3
---


# Comments for Jack

Jack, I'm sharing the random forest model with you that predicts 5 categories of metacritic scores. I haven't performed feature selection/dimensionality reduction yet. We should reduce the number of features and select the most important ones. If possible, consider applying PCA. Keep in mind our goal of identifying the features that strongly predict metacritic scores. It would be fantastic if you can find the top 10 features that greatly influence the model. Additionally, if you have extra time, please work on further improving the model. Increase the cross-fold validation to 10 (but wait until the very end as it takes about 7 minutes with 5 folds). Explore enhancing the hyperparameters and consider adding more features. The NLP method I converted from R has ample room for feature engineering, so go for it. If we can achieve a Kappa of .90 and explain which features are responsible for that, we will be in a good position.



## Libaries for data manipulation, analysis, visualization, natural language processing, and machine learning using various libraries.
```{python}
# Importing the pandas library for data manipulation and analysis
import pandas as pd

# Importing the NumPy library for numerical operations
import numpy as np

# Importing the seaborn library for data visualization
import seaborn as sns

# Importing the matplotlib library for creating plots and charts
import matplotlib.pyplot as plt

# Importing the nltk library for natural language processing tasks
import nltk

# Importing the re module for regular expressions
import re

# Importing stopwords from the nltk.corpus module
from nltk.corpus import stopwords

# Importing the WordNetLemmatizer from the nltk.stem module
from nltk.stem import WordNetLemmatizer

# Importing Counter from the collections module
from collections import Counter

# Importing train_test_split for splitting the data into training and testing sets
from sklearn.model_selection import train_test_split

# Importing the MinMaxScaler for feature scaling
from sklearn.preprocessing import MinMaxScaler

# Importing the RandomForestClassifier from the sklearn.ensemble module
from sklearn.ensemble import RandomForestClassifier

# Importing evaluation metrics from the sklearn.metrics module
from sklearn.metrics import (
    cohen_kappa_score,
    roc_auc_score,
    accuracy_score,
    precision_score,
    recall_score,
    confusion_matrix,
)

# Importing GridSearchCV from the sklearn.model_selection module
from sklearn.model_selection import GridSearchCV
```

## Data Preprocessing and Analysis

These operations demonstrate common data preprocessing tasks such as reading data, removing columns, handling missing values, creating new columns, converting data types, and performing basic analysis on the data.

1. The CSV file "gamesMetacritic.csv" is read into a DataFrame called games, and the shape of the DataFrame is printed.

2. Columns with missing values are identified and stored in the columns_with_missing_values list.

3. The number of missing values is printed for each column with missing values.

4. The missing values are dropped from the DataFrame using the `drop

```{python}
# Read the CSV file into a DataFrame
games = pd.read_csv("Final.csv")
print(games.shape)

# Define rating ranges and corresponding labels
rating_ranges = [(0, 67, "Poor"),
                 (68, 72, "Mixed"),
                 (73, 76, "Good"),
                 (77, 80, "Great"),
                 (81, 84, "Excellent"),
                 (85, 100, "Masterpiece")]

# Create a new column for the categorical labels
games["metacritic_category"] = pd.cut(
    games["metacritic"],
    bins=[r[0] for r in rating_ranges] + [101],
    labels=[r[2] for r in rating_ranges],
    right=False
)

# Convert "released" column to datetime type
games["released"] = pd.to_datetime(games["released"])

def get_season(date):
    if date.month in (3, 4, 5):
        return "Spring"
    elif date.month in (6, 7, 8):
        return "Summer"
    elif date.month in (9, 10, 11):
        return "Fall"
    elif date.month in (12, 1, 2):
        return "Winter"
    else:
        return "Invalid date"

# Apply the get_season function to the "released" column and create the "season" column
games["season"] = games["released"].apply(get_season)

# Print the first 5 rows with the new "season" column
print(games.head(5))
print("Shape:", games.shape)

# Define the playtime ranges and corresponding categories
playtime_ranges = [(0, 10), (11, 50), (51, 100), (101, float('inf'))]
playtime_categories = ['Novice', 'Casual', 'Experienced', 'Veteran']

# Create a new column with categorical values based on playtime
games['playtime_category'] = pd.cut(
    games['playtime'],
    bins=[range[0] - 1 for range in playtime_ranges] + [playtime_ranges[-1][1]],
    labels=playtime_categories
)


# Find the columns with missing values
columns_with_missing_values = games.columns[games.isna().any()].tolist()

# Print the number of missing values for each column
print("\nNumber of missing values per column:")
for column in columns_with_missing_values:
    missing_count = games[column].isna().sum()
    print(f"{column}: {missing_count}")

# Drop the missing values
games.dropna(inplace=True)

# Print the number of missing values for each column
print("\nNumber of missing values per column:")
for column in columns_with_missing_values:
    missing_count = games[column].isna().sum()
    print(f"{column}: {missing_count}")

```

```{python}
# Clean up the genres column
games['genres'] = games['genres'].str.replace('\[.*?\]', '')  # Remove anything within square brackets [...]
games['genres'] = games['genres'].str.replace('\(.*?\)', '')  # Remove anything within parentheses (...)
games['genres'] = games['genres'].str.strip()  # Remove leading/trailing whitespace

# Split the genres and create new rows
new_rows = []
for index, row in games.iterrows():
    genres = row["genres"].split(",")
    for genre in genres:
        genre = genre.strip()
        if "-" in genre:
            genre_parts = genre.split("-")
            genre_parts = [part.capitalize() for part in genre_parts]
            genre = "-".join(genre_parts)
        elif "\n" in genre:
            genre_parts = genre.split("\n")
            genre_parts = [part.capitalize() for part in genre_parts]
            genre = " ".join(genre_parts)
        else:
            genre = genre.capitalize()
        new_row = {"subgenres": genre}
        new_rows.append(new_row)

# Create a new DataFrame from the new rows
new_games = pd.DataFrame(new_rows)

# Drop the original "genres" column
games.drop(columns=["genres"], inplace=True)

# Add the "subgenres" column to the original DataFrame
games = pd.concat([games, new_games], axis=1)

# Capitalize the "subgenres" column
games['subgenres'] = games['subgenres'].str.title()

# Display the updated dataset
print(games)

# Capitalize the "subgenres" column again
games['subgenres'] = games['subgenres'].str.title()

# Display the unique values in the "subgenres" column
print(games.subgenres.unique())
print(games.subgenres.nunique())

games.to_csv("genre.csv", index=False)


# Find the columns with missing values
columns_with_missing_values = games.columns[games.isna().any()].tolist() 

# Print the number of missing values for each column
print("\nNumber of missing values per column:")
for column in columns_with_missing_values:
    missing_count = games[column].isna().sum()
    print(f"{column}: {missing_count}")

# Drop the missing values
games.dropna(inplace=True)

# Print the number of missing values for each column
print("\nNumber of missing values per column:")
for column in columns_with_missing_values:
    missing_count = games[column].isna().sum()
    print(f"{column}: {missing_count}")
```

## Feature Engineering

The code block performs the conversion of specified categorical columns into dummy variables in the modelDs DataFrame. The columns to be converted are specified in the columns_to_dummy list. Using the pd.get_dummies() function, dummy variables are created for each unique value in these columns, with column names prefixed by the original column names. The drop_first=True parameter ensures that the first dummy variable for each column is dropped to avoid multicollinearity. The resulting dummy variables are then concatenated with the original modelDs DataFrame using pd.concat(). Finally, the original categorical columns are dropped from the modelDs DataFrame using drop() with the axis=1 parameter. This process transforms categorical variables into numerical representations, making them suitable for machine learning algorithms that require numeric input.

```{python}
# INDICATOR Variables
# ADD FEATURE: platform, publisher, developer counts/mean/median
# Look into ratios given word actions: active/passive aggressive/simple


# ADD FEATURE: Single Platform, Multi-Platform, etc.

# Step 1: Count the number of unique platform names for each game
platform_counts = games.groupby('name')['platform_name'].nunique()

# Step 2: Create a new column called 'Platform_Type' and initialize it with an empty string
games['platform_type'] = ''

# Step 3: Assign 'Single-Platform' to games with only one platform name
single_platform_games = platform_counts[platform_counts == 1].index
games.loc[games['name'].isin(single_platform_games), 'platform_type'] = 'Single-Platform'

# Step 4: Assign 'Multi-Platform' to games with more than one platform name
multi_platform_games = platform_counts[platform_counts > 1].index
games.loc[games['name'].isin(multi_platform_games), 'platform_type'] = 'Multi-Platform'


# Step 1: Convert the 'released' column to datetime type
games['released'] = pd.to_datetime(games['released'])

# Step 2: Extract the year and create a new column 'year'
games['year'] = games['released'].dt.year



# ADD FEATURE: metacritic trend line

# Step 1: Group the data by year and category, calculate the average score
grouped = games.groupby(['year', 'metacritic_category']).agg(avg_score=('metacritic', 'mean')).reset_index()

# Step 2: Calculate the year-over-year difference in scores
grouped['yoy_diff'] = grouped.groupby('metacritic_category')['avg_score'].diff()

# Step 3: Create the trend line column
grouped['trend_line_metacritic_yoy'] = ''

# Step 4: Assign the trend line values based on the year-over-year difference
grouped.loc[grouped['yoy_diff'] > 0, 'trend_line_metacritic_yoy'] = 'Positive'
grouped.loc[grouped['yoy_diff'] < 0, 'trend_line_metacritic_yoy'] = 'Negative'
grouped.loc[grouped['yoy_diff'] == 0, 'trend_line_metacritic_yoy'] = 'Stable'

# Merge the trend line column back into the 'games' DataFrame
games = pd.merge(games, grouped[['year', 'metacritic_category', 'trend_line_metacritic_yoy']], on=['year', 'metacritic_category'], how='left')


# ADD FEATURE: playtime trend line


# Step 1: Group the data by year and calculate the average playtime
grouped = games.groupby('year')['playtime'].mean().reset_index()

# Step 2: Calculate the year-over-year difference in playtime
grouped['yoy_diff'] = grouped['playtime'].diff()

# Step 3: Create the trend line column
grouped['trend_line_playtime_yoy'] = ''

# Step 4: Assign the trend line values based on the year-over-year difference
grouped.loc[grouped['yoy_diff'] > 0, 'trend_line_playtime_yoy'] = 'Increasing'
grouped.loc[grouped['yoy_diff'] < 0, 'trend_line_playtime_yoy'] = 'Decreasing'
grouped.loc[grouped['yoy_diff'] == 0, 'trend_line_playtime_yoy'] = 'Stable'

# Merge the trend line column back into the 'games' DataFrame
games = pd.merge(games, grouped[['year', 'trend_line_playtime_yoy']], on='year', how='left')

# ADD FEATURES: playtime quartiles

# Step 1: Calculate the quartiles for playtime
quartiles = np.linspace(0, 1, num=5)  # Split into quartiles (0%, 25%, 50%, 75%, 100%)

# Step 2: Create the playtime quartile labels
quartile_labels = ['Q1', 'Q2', 'Q3', 'Q4']

# Step 3: Assign quartile labels to playtime quartiles
games['playtime_quartile'] = pd.qcut(games['playtime'], quartiles, labels=quartile_labels, duplicates='drop')

# Fill missing values with a default label if any
games['playtime_quartile'] = games['playtime_quartile'].cat.add_categories('N/A').fillna('N/A')


# ADD FEATURES: playtime ratios

# Step 1: Calculate the maximum playtime in the dataset
max_playtime = games['playtime'].max()

# Step 2: Create the playtime ratio feature
games['playtime_ratio'] = games['playtime'] / max_playtime


# ADD FEATURES: interactions variables

# Interaction feature: genre by year
games['genre_year'] = games['genre'] + '_' + games['year'].astype(str)

# Interaction feature: subgenres by year
games['subgenres_year'] = games['subgenres'] + '_' + games['year'].astype(str)

# Interaction feature: platform by year
games['platform_year'] = games['platform_name'] + '_' + games['year'].astype(str)

# Interaction feature: publishers by year
games['publishers_year'] = games['publishers'] + '_' + games['year'].astype(str)

# Interaction feature: developers by year
games['developers_year'] = games['developers'] + '_' + games['year'].astype(str)


games.head(10)
```

```{python}

# Remove the specified columns
columns_to_remove = ['id', 'background_image', 'metacritic', 'name', 'playtime', 'description', 'released']
games.drop(columns_to_remove, axis=1, inplace=True)
print(games.shape)

```


```{python}
# Calculate the total number of rows per genre
genre_counts = games['genre'].value_counts()

# Display the genre counts
print(genre_counts)
print(games)


Action = 2982
Adventure = 2063
RPG = 1456
Shooter = 1391
Strategy = 788
Simulation = 643

# Calculate the total number of games
total_games = Action + Adventure + RPG + Shooter + Strategy + Simulation
total_games = 2982 + 2063 + 1456 + 1391 + 788 + 643
total_games = 9323

# Calculate the weights for each genre
weight_Action = Action / total_games
weight_Adventure = Adventure / total_games
weight_RPG = RPG / total_games
weight_Shooter = Shooter / total_games
weight_Strategy = Strategy / total_games
weight_Simulation = Simulation / total_games

print("Weights:")
print("Action:", weight_Action)
print("Adventure:", weight_Adventure)
print("RPG:", weight_RPG)
print("Shooter:", weight_Shooter)
print("Strategy:", weight_Strategy)
print("Simulation:", weight_Simulation)

modelDs = games
```

## Dummy Columns

```{python}

columns_to_dummy = ['platform_name', 'developers', 'publishers', 'esrb_rating_name',
                    'gamemodes', 'genre', 'season',
                    'playtime_category', 'subgenres', 'platform_type', 'year',
                    'trend_line_metacritic_yoy', 'trend_line_playtime_yoy',
                    'playtime_quartile', 'genre_year', 'subgenres_year',
                    'platform_year', 'publishers_year', 'developers_year']

print("Shape of modelDs DataFrame:", modelDs.shape)
print("Number of columns in modelDs DataFrame:", modelDs.shape[1])
print("Expected number of columns:", len(columns_to_dummy))
print("Columns to dummy:", columns_to_dummy)

prefix = columns_to_dummy[:-1]  # Remove the last element from the prefix list

dummy_cols = pd.get_dummies(modelDs[columns_to_dummy], prefix=prefix, drop_first=True)
modelDs = pd.concat([modelDs, dummy_cols], axis=1)
modelDs.drop(columns_to_dummy, axis=1, inplace=True)

modelDs
```


## Random Forest Classifier with Hyperparameter Tuning and Evaluation

The code block involves the following steps for building and evaluating a model:

1. The data is split into training and testing sets using the `train_test_split()` function. The features (input variables) are stored in `X`, and the target variable is stored in `y`.
2. A parameter grid is defined to specify different combinations of hyperparameters for tuning. The hyperparameters considered are the number of trees in the forest (`n_estimators`), the maximum depth of each tree (`max_depth`), and the minimum number of samples required to split an internal node (`min_samples_split`).
3. A random forest classifier is created using `RandomForestClassifier()`.
4. Grid search with cross-validation is performed using `GridSearchCV()` to find the best combination of hyperparameters. The grid search is conducted on the training set.
5. The best hyperparameters are retrieved using `best_params_`.
6. The best model is obtained using `best_estimator_` from the grid search results.
7. Predictions are made on the testing set using the best model.
8. Class probabilities are calculated using `predict_proba()`.
9. The ROC AUC score is calculated using `roc_auc_score()`.
10. The Kappa coefficient is calculated using `cohen_kappa_score()`.
11. Accuracy is calculated using `accuracy_score()`.
12. Precision is calculated using `precision_score()`.
13. Recall is calculated using `recall_score()`.
14. The confusion matrix is calculated using `confusion_matrix()`.

These steps involve training and evaluating a random forest classifier with hyperparameter tuning using grid search. The resulting model's performance is assessed using various evaluation metrics such as ROC AUC score, Kappa coefficient, accuracy, precision, recall, and the confusion matrix.


```{python}
X = modelDs.drop('metacritic_category', axis=1)  # Features (input variables)
y = modelDs['metacritic_category']  # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=154)

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': 300,  # Number of trees in the forest
    'max_depth': None,  # Maximum depth of each tree
    'min_samples_split': 5  # Minimum number of samples required to split an internal node
}

# Create a random forest classifier with specific hyperparameters
classifier = RandomForestClassifier(**param_grid)

# Fit the classifier to the training data
classifier.fit(X_train, y_train)

# Use the trained classifier for predictions
y_pred = classifier.predict(X_test)

# Make predictions with class probabilities
y_pred_prob = classifier.predict_proba(X_test)

# Calculate ROC AUC score
roc_auc = roc_auc_score(y_test, y_pred_prob, multi_class='ovr')
print(f"ROC AUC: {roc_auc}")

# Calculate Kappa coefficient
kappa = cohen_kappa_score(y_test, y_pred)
print(f"Kappa: {kappa}")

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Calculate precision
precision = precision_score(y_test, y_pred, average='weighted')
print(f"Precision: {precision}")

# Calculate recall
recall = recall_score(y_test, y_pred, average='weighted')
print(f"Recall: {recall}")

# Calculate confusion matrix
confusion = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(confusion)
```